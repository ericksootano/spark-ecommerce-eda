# ğŸ“Š E-Commerce Data Analysis & Engineering con PySpark

![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![SQL](https://img.shields.io/badge/SQL-CC2927?style=for-the-badge&logo=microsoftsqlserver&logoColor=white)

> **Un proyecto End-to-End de IngenierÃ­a de Datos y AnÃ¡lisis Exploratorio (EDA) simulando un entorno de Big Data.**

## ğŸ“– DescripciÃ³n del Proyecto

Este repositorio contiene el flujo de trabajo completo para el procesamiento y anÃ¡lisis de un dataset transaccional de E-Commerce real (~540,000 registros). El objetivo principal fue transformar datos crudos y "sucios" en un **Dashboard Ejecutivo** accionable, utilizando la potencia de procesamiento distribuido de **Apache Spark (PySpark)** en la plataforma **Databricks Free Edition**.

El proyecto aborda desde la ingesta y limpieza de datos hasta la ingenierÃ­a de caracterÃ­sticas compleja y la visualizaciÃ³n de estrategias de negocio.

---

## ğŸš€ Dashboard Ejecutivo

*(AquÃ­ puedes colocar la captura de pantalla de tu Dashboard completo)*
![Dashboard Preview](![Dashboard](1.DashboardE-commerce.png)

---

## âš™ï¸ Arquitectura y TecnologÃ­as

* **Plataforma:** Databricks (Spark 4.0)
* **Lenguajes:** Python (PySpark) y Spark SQL.
* **Limpieza de Datos:** Manejo de formatos de fecha inconsistentes (`try_to_timestamp`, `coalesce`), imputaciÃ³n de nulos y eliminaciÃ³n de duplicados.
* **IngenierÃ­a de CaracterÃ­sticas:** CreaciÃ³n de mÃ©tricas temporales (Year-Month) y financieras (Revenue por lÃ­nea).
* **AnÃ¡lisis Avanzado:** SegmentaciÃ³n RFM (Recencia, Frecuencia, Monetario) utilizando **Window Functions** y **CTEs**.

---

## ğŸ’¡ Insights de Negocio Clave

Tras procesar los datos, se descubrieron los siguientes patrones estratÃ©gicos:

| Insight | DescripciÃ³n | Impacto |
| :--- | :--- | :--- |
| **ğŸ‘‘ Dominio del Reino Unido** | El **~90%** de los ingresos provienen de UK. | Riesgo alto de dependencia de un solo mercado. Se recomienda expansiÃ³n a Alemania/Francia. |
| **ğŸ“… La "Hora Dorada"** | El 80% de las transacciones ocurren entre **10:00 AM y 3:00 PM** (Lun-Jue). | Ventana crÃ­tica para soporte al cliente y campaÃ±as de marketing. Evitar mantenimientos en este horario. |
| **ğŸ’ SegmentaciÃ³n VIP** | Un pequeÃ±o grupo de clientes **"Champions"** (Score 4-4-4) genera la mayor parte del valor. | Prioridad absoluta en retenciÃ³n. Es mÃ¡s rentable fidelizarlos que adquirir nuevos. |
| **ğŸ“ˆ Estacionalidad** | Pico dramÃ¡tico de ventas en **Noviembre**. | La planificaciÃ³n de inventario para Q4 debe comenzar en Septiembre. |

---

## ğŸ› ï¸ DesafÃ­os TÃ©cnicos Superados

### 1. Fechas en Spark 4.0
El dataset presentaba formatos mixtos (`M/d/yyyy` vs `MM/dd/yyyy`) que causaban fallos en el pipeline.
**SoluciÃ³n:** ImplementaciÃ³n de una lÃ³gica de coalescencia robusta:

```python
# Snippet de la soluciÃ³n
df_cleaned = df_cleaned.withColumn(
    "InvoiceDate",
    F.coalesce(
        F.try_to_timestamp(F.col("InvoiceDate"), F.lit("M/d/yyyy H:m")),
        F.try_to_timestamp(F.col("InvoiceDate"), F.lit("MM/dd/yyyy HH:mm"))
    )
)
```

### 2. Variables SQL en Spark 4.0
La nueva versiÃ³n de Spark maneja diferente la inyecciÃ³n de variables en SQL (SET variable...). SoluciÃ³n: Uso de f-strings de Python para inyectar parÃ¡metros dinÃ¡micos (como la fecha de corte para el anÃ¡lisis RFM) directamente en las consultas SQL.

## ğŸ“Š Visualizaciones Destacadas
Mapa de Calor (Patrones de Compra)
Muestra la concentraciÃ³n de ventas por dÃ­a y hora.

### Top Productos (Pareto)
IdentificaciÃ³n de los Best Sellers.

ğŸ“‚ Estructura del Repositorio
- EDA_ECommerce_Project.dbc: Archivo nativo de Databricks (incluye datos, cÃ³digo y dashboard).

- EDA_ECommerce_Project.ipynb: VersiÃ³n Jupyter Notebook para visualizaciÃ³n en GitHub.

- README.md: DocumentaciÃ³n del proyecto.

ğŸ‘¨â€ğŸ’» Autor
Erickson OtaÃ±o Ingeniero de Datos | Cloud Data Platforms

Este proyecto fue realizado como parte de una prÃ¡ctica intensiva de procesamiento de datos a gran escala.
